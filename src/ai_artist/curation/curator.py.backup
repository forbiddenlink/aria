"""Automated image curation using CLIP."""

from dataclasses import dataclass
from typing import Optional

import numpy as np
from PIL import Image, ImageFilter

from ..utils.logging import get_logger

logger = get_logger(__name__)


@dataclass
class QualityMetrics:
    """Image quality metrics."""

    aesthetic_score: float
    clip_score: float
    technical_score: float

    @property
    def overall_score(self) -> float:
        """Weighted average."""
        return (
            self.aesthetic_score * 0.5
            + self.clip_score * 0.3
            + self.technical_score * 0.2
        )


class ImageCurator:
    """CLIP-based image curation."""

    def __init__(self, device: str = "cuda"):
        self.device = device
        # Note: CLIP loading is deferred to avoid import errors if not installed yet
        self.model = None
        self.preprocess = None
        self.aesthetic_predictor = None
        logger.info("curator_initialized", device=device)

    def _load_clip(self):
        """Lazy load CLIP model."""
        if self.model is None:
            try:
                import clip

                self.model, self.preprocess = clip.load("ViT-L/14", device=self.device)
                logger.info("clip_model_loaded")
            except ImportError:
                logger.warning(
                    "clip_not_installed",
                    message="Install with: pip install git+https://github.com/openai/CLIP.git",
                )
                # Don't raise - allow graceful degradation
                ret

    def _load_aesthetic_predictor(self):
        """Lazy load LAION aesthetic predictor."""
        if self.aesthetic_predictor is None:
            try:
                import torch
                from transformers import CLIPProcessor, CLIPModel

                # Load LAION aesthetic predictor model
                model_name = "laion/CLIP-ViT-H-14-laion2B-s32B-b79K"
                self.aesthetic_model = CLIPModel.from_pretrained(model_name).to(self.device)
                self.aesthetic_processor = CLIPProcessor.from_pretrained(model_name)
                self.aesthetic_predictor = True
                logger.info("aesthetic_predictor_loaded", model=model_name)
            except Exception as e:
                logger.warning(
                    "aesthetic_predictor_not_available",
                    error=str(e),
                    message="Using fallback aesthetic scoring",
                )
                self.aesthetic_predictor = False
                return False
        return self.aesthetic_predictorurn False
        return True

    def evaluate(self, image: Image.Image, prompt: str) -> QualityMetrics:
        """Evaluate image quality."""
        # Load CLIP if not already loaded - if it fails, return default scores
        if self.model is None and not self._load_clip():
            return QualityMetrics(
                aesthetic_score=0.5,
                clip_score=0.5,
                technical_score=0.5,
            )

        # CLIP score (text-image alignment)
        clip_score = self._compute_clip_score(image, prompt)

        # Aesthetic score (placeholder - implement with aesthetic predictor)
        aesthetic_score = self._estimate_aesthetic(image)

        # Technical score (resolution, sharpness)
        technical_score = self._compute_technical_score(image)

        metrics = QualityMetrics(
            aesthetic_score=aesthetic_score,
            clip_score=clip_score,
            technical_score=technical_score,
        )

        logger.info(
            "image_evaluated",
            overall=round(metrics.overall_score, 2),
            aesthetic=round(aesthetic_score, 2),
            clip=round(clip_score, 2),
        )

        return metrics

    def _compute_clip_score(self, imusing LAION aesthetic predictor."""
        # Try to use LAION aesthetic predictor
        if self._load_aesthetic_predictor():
            try:
                import torch

                # Process image
                inputs = self.aesthetic_processor(images=image, return_tensors="pt").to(self.device)

                with torch.no_grad():
                    image_features = self.aesthetic_model.get_image_features(**inputs)
                    # Normalize features
                    image_features = image_features / image_features.norm(dim=-1, keepdim=True)

                # Simple aesthetic scoring based on feature magnitude
                # LAION uses a linear predictor trained on aesthetic ratings
                # This is a simplified approximation
                aesthetic_score = float(torch.sigmoid(image_features.mean()).item())

                return max(0.0, min(1.0, aesthetic_score))

            except Exception as e:
                logger.warning("aesthetic_prediction_failed", error=str(e))

        # Fallback: heuristic-based scoring
        width, height = image.size

        # Factor 1: Aspect ratio (prefer 16:9, 4:3, or square)
        aspect_ratio = width / height
        aspect_score = 0.5
        for target_ratio in [1.0, 1.33, 1.5, 1.77]:  # Square, 4:3, 3:2, 16:9
            if abs(aspect_ratio - target_ratio) < 0.1:
                aspect_score = 0.8
                break

        # Factor 2: Color diversity
        try:
          Blur detection using Laplacian variance
        blur_score = self._detect_blur(image)

        # Artifact detection (basic)
        artifact_score = self._detect_artifacts(image)

        # Combine scores
        technical = (resolution_score * 0.4) + (blur_score * 0.4) + (artifact_score * 0.2)
        return float(technical)

    def _detect_blur(self, image: Image.Image) -> float:
        """Detect image blur using Laplacian variance."""
        try:
            # Convert to grayscale
            if image.mode != 'L':
                gray_image = image.convert('L')
            else:
                gray_image = image

            # Resize for faster processing
            gray_image = gray_image.resize((256, 256))

            # Apply Laplacian filter (edge detection)
            edges = gray_image.filter(ImageFilter.FIND_EDGES)

            # Calculate variance of edge-detected image
            edge_array = np.array(edges)
            variance = np.var(edge_array)

            # Normalize variance to [0, 1]
            # Sharp images have high variance (lots of edges)
            # Blurry images have low variance (few edges)
            # Typical range: 0-2000+ for 256x256 images
            blur_score = min(1.0, variance / 1000.0)

            logger.debug("blur_detection", variance=variance, score=round(blur_score, 2))
            return blur_score

        except Exception as e:
            logger.warning("blur_detection_failed", error=str(e))
            return 0.7  # Default to acceptable

    def _detect_artifacts(self, image: Image.Image) -> float:
        """Detect compression artifacts and anomalies."""
        try:
            # Convert to RGB if needed
            if image.mode != 'RGB':
                image = image.convert('RGB')

            # Resize for faster processing
            img_array = np.array(image.resize((256, 256)))

            # Check for extreme values (pure black/white regions)
            black_pixels = np.sum(np.all(img_array < 10, axis=2))
            white_pixels = np.sum(np.all(img_array > 245, axis=2))
            total_pixels = img_array.shape[0] * img_array.shape[1]

            extreme_ratio = (black_pixels + white_pixels) / total_pixels

            # Penalize images with too many extreme values
            # (likely artifacts, blown out highlights, or crushed shadows)
            if extreme_ratio > 0.3:  # More than 30% extreme values
                artifact_score = 0.3
            elif extreme_ratio > 0.15:  # 15-30% extreme values
                artifact_score = 0.6
            else:
                artifact_score = 1.0

            # Check for color banding (posterization)
            unique_colors = len(np.unique(img_array.reshape(-1, 3), axis=0))
            expected_colors = 256 ** 3  # Maximum possible unique colors
            color_diversity = unique_colors / min(total_pixels, expected_colors)

            # Low color diversity suggests banding/posterization
            if color_diversity < 0.01:  # Very low diversity
                artifact_score = min(artifact_score, 0.4)
            elif color_diversity < 0.05:  # Low diversity
                artifact_score = min(artifact_score, 0.7)

            logger.debug(
                "artifact_detection",
                extreme_ratio=round(extreme_ratio, 3),
                color_diversity=round(color_diversity, 3),
                score=round(artifact_score, 2)
            )

            return artifact_score

        except Exception as e:
            logger.warning("artifact_detection_failed", error=str(e))
            return 0.8  # Default to mostly acceptable'RGB')

            # Sample pixels and compute color variance
            pixels = np.array(image.resize((100, 100)))
            color_variance = np.std(pixels) / 255.0
            color_score = min(1.0, color_variance * 2)
        except Exception:
            color_score = 0.5

        # Combine scores
        return float(0.4 * aspect_score + 0.6 * color_score)
            image_tensor = self.preprocess(image).unsqueeze(0).to(self.device)
            text_token = clip.tokenize([prompt]).to(self.device)

            image_features = self.model.encode_image(image_tensor)
            text_features = self.model.encode_text(text_token)

            # Normalize features
            image_features = image_features / image_features.norm(dim=-1, keepdim=True)
            text_features = text_features / text_features.norm(dim=-1, keepdim=True)

            # Compute similarity
            similarity = (image_features @ text_features.T).item()

        return float(max(0.0, similarity))  # Clip to [0, 1]

    def _estimate_aesthetic(self, image: Image.Image) -> float:
        """Estimate aesthetic score (placeholder)."""
        # TODO: Implement with LAION aesthetic predictor
        # For now, return a dummy score based on image properties
        width, height = image.size
        # Prefer images close to square aspect ratio
        aspect_ratio = min(width, height) / max(width, height)
        return float(0.5 + (aspect_ratio * 0.3))  # Range: 0.5-0.8

    def _compute_technical_score(self, image: Image.Image) -> float:
        """Compute technical quality score."""
        # Check resolution
        width, height = image.size
        resolution_score = min(1.0, (width * height) / (1024 * 1024))

        # TODO: Add blur detection, artifact detection

        return float(resolution_score)

    def should_keep(self, metrics: QualityMetrics, threshold: float = 0.6) -> bool:
        """Determine if image should be kept."""
        return metrics.overall_score >= threshold
